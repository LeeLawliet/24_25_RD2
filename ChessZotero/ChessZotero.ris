TY  - GEN
TI  - Amortized Planning with Large-Scale Transformers: A Case Study on Chess
AU  - Ruoss, Anian
AU  - Delétang, Grégoire
AU  - Medapati, Sourabh
AU  - Grau-Moya, Jordi
AU  - Wenliang, Li Kevin
AU  - Catt, Elliot
AU  - Reid, John
AU  - Lewis, Cannada A.
AU  - Veness, Joel
AU  - Genewein, Tim
AB  - This paper uses chess, a landmark planning problem in AI, to assess transformers' performance on a planning task where memorization is futile $\unicode{x2013}$ even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points) provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.
DA  - 2024/10/21/
PY  - 2024
DO  - 10.48550/arXiv.2402.04494
DP  - arXiv.org
PB  - arXiv
ST  - Amortized Planning with Large-Scale Transformers
UR  - http://arxiv.org/abs/2402.04494
Y2  - 2025/05/05/06:36:22
L1  - files/6/Ruoss et al. - 2024 - Amortized Planning with Large-Scale Transformers A Case Study on Chess.pdf
L2  - files/7/2402.html
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - Chess AI: Competing Paradigms for Machine Intelligence
AU  - Maharaj, Shiva
AU  - Polson, Nick
AU  - Turk, Alex
T2  - Entropy
AB  - Endgame studies have long served as a tool for testing human creativity and intelligence. We find that they can serve as a tool for testing machine ability as well. Two of the leading chess engines, Stockfish and Leela Chess Zero (LCZero), employ significantly different methods during play. We use Plaskett's Puzzle, a famous endgame study from the late 1970s, to compare the two engines. Our experiments show that Stockfish outperforms LCZero on the puzzle. We examine the algorithmic differences between the engines and use our observations as a basis for carefully interpreting the test results. Drawing inspiration from how humans solve chess problems, we ask whether machines can possess a form of imagination. On the theoretical side, we describe how Bellman's equation may be applied to optimize the probability of winning. To conclude, we discuss the implications of our work on artificial intelligence (AI) and artificial general intelligence (AGI), suggesting possible avenues for future research.
DA  - 2022/04/14/
PY  - 2022
DO  - 10.3390/e24040550
DP  - arXiv.org
VL  - 24
IS  - 4
SP  - 550
J2  - Entropy
SN  - 1099-4300
ST  - Chess AI
UR  - http://arxiv.org/abs/2109.11602
Y2  - 2025/05/05/06:36:48
L1  - files/10/Maharaj et al. - 2022 - Chess AI Competing Paradigms for Machine Intelligence.pdf
L2  - files/11/2109.html
N1  - Comment: 15 pages, 8 figures
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Neural Networks for Chess
AU  - Klein, Dominik
AB  - AlphaZero, Leela Chess Zero and Stockfish NNUE revolutionized Computer Chess. This book gives a complete introduction into the technical inner workings of such engines. The book is split into four main chapters -- excluding chapter 1 (introduction) and chapter 6 (conclusion): Chapter 2 introduces neural networks and covers all the basic building blocks that are used to build deep networks such as those used by AlphaZero. Contents include the perceptron, back-propagation and gradient descent, classification, regression, multilayer perceptron, vectorization techniques, convolutional networks, squeeze and excitation networks, fully connected networks, batch normalization and rectified linear units, residual layers, overfitting and underfitting. Chapter 3 introduces classical search techniques used for chess engines as well as those used by AlphaZero. Contents include minimax, alpha-beta search, and Monte Carlo tree search. Chapter 4 shows how modern chess engines are designed. Aside from the ground-breaking AlphaGo, AlphaGo Zero and AlphaZero we cover Leela Chess Zero, Fat Fritz, Fat Fritz 2 and Efficiently Updatable Neural Networks (NNUE) as well as Maia. Chapter 5 is about implementing a miniaturized AlphaZero. Hexapawn, a minimalistic version of chess, is used as an example for that. Hexapawn is solved by minimax search and training positions for supervised learning are generated. Then as a comparison, an AlphaZero-like training loop is implemented where training is done via self-play combined with reinforcement learning. Finally, AlphaZero-like training and supervised training are compared.
DA  - 2022/09/03/
PY  - 2022
DO  - 10.48550/arXiv.2209.01506
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.01506
Y2  - 2025/05/05/06:37:00
L1  - files/13/Klein - 2022 - Neural Networks for Chess.pdf
L2  - files/14/2209.html
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

